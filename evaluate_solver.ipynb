{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import h5py\n",
    "import datetime\n",
    "\n",
    "import poselib\n",
    "from relscalenet.models.relscale_cache import RelScaleNetCached, write_hdf5\n",
    "from relscalenet.dataset_reader import EvaluationDataset\n",
    "import relscalenet.geometry as geom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOLVER = \"ours\"  # ours or 5pt\n",
    "KEYPOINTS = \"spsg\"\n",
    "\n",
    "weights_path = \"weights/model_final.pth\"\n",
    "data_path = f\"data/scannet1500_{KEYPOINTS}.h5\"\n",
    "images_path = \"data/scannet1500-images/images\"\n",
    "cache_path = f\"{KEYPOINTS}_relscale_cache.h5\"\n",
    "\n",
    "pose_estimates_path = f\"{KEYPOINTS}_pose_estimates_scannet.h5\"\n",
    "\n",
    "RANSAC_OPT ={\n",
    "    'max_epipolar_error': 1.5,\n",
    "    'min_iterations': 1000,\n",
    "    'max_iterations': 100000,\n",
    "    'success_prob': 0.9999,\n",
    "    'dyn_num_trials_mult': 3,\n",
    "}\n",
    "BUNDLE_OPT = {\n",
    "    'loss_scale': 1.0,\n",
    "    'loss_type': 'TRIVIAL'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run pose estimation\n",
    "Note that this runs faster if depths are pre-computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eaef32882e745cdaf85fd8a1fb43612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estimating poses:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated pose median inlier ratio: 0.5421123747325751\n",
      "Saving results to spsg_pose_estimates_scannet.h5...\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "relscalenet = RelScaleNetCached(weights_path, device)\n",
    "relscalenet.load_from_h5(cache_path)\n",
    "\n",
    "dataset = EvaluationDataset(data_path, images_path)\n",
    "\n",
    "pose_estimates = {}\n",
    "for pair in tqdm(dataset, desc=\"Estimating poses\"):\n",
    "\n",
    "    x1, x2 = pair.matches()\n",
    "    R_gt, t_gt = pair.relative_pose()\n",
    "\n",
    "    if len(x1) < 5:\n",
    "        # Not enough inliers to calculate pose; skip to next image pair\n",
    "        pose_estimates[pair.key] = {\n",
    "            'R_gt': R_gt, 't_gt': t_gt,\n",
    "            'R_est': None, 't_est': None,\n",
    "            'runtime': np.nan,\n",
    "            'inl_ratio': 0.0,\n",
    "        }\n",
    "        continue\n",
    "\n",
    "    # Predict relative scale (or load from cache)\n",
    "    im1_path, im2_path = pair.image_paths()\n",
    "    relscale = relscalenet.predict_image_pair(im1_path, im2_path, x1, x2)\n",
    "\n",
    "    # Convert relative scale to relative depth\n",
    "    cam1, cam2 = pair.cameras()\n",
    "    f1 = cam1['params'][0:2].mean()\n",
    "    f2 = cam2['params'][0:2].mean()\n",
    "    reldepth = (relscale * f2 / f1).flatten()\n",
    "\n",
    "    # Run pose estimation\n",
    "    if SOLVER == \"ours\":\n",
    "        tt1 = datetime.datetime.now()\n",
    "        estimated_pose, info = poselib.estimate_relative_pose_w_relative_depth(x1, x2, reldepth, cam1, cam2, RANSAC_OPT, BUNDLE_OPT)\n",
    "        tt2 = datetime.datetime.now()\n",
    "    elif SOLVER == \"5pt\":\n",
    "        tt1 = datetime.datetime.now()\n",
    "        estimated_pose, info = poselib.estimate_relative_pose(x1, x2, cam1, cam2, RANSAC_OPT, BUNDLE_OPT)\n",
    "        tt2 = datetime.datetime.now()\n",
    "    runtime = (tt2-tt1).total_seconds()\n",
    "\n",
    "    # Store results\n",
    "    pose_estimates[pair.key] = {\n",
    "        'R_gt': R_gt, 't_gt': t_gt,\n",
    "        'R_est': estimated_pose.R, 't_est': estimated_pose.t,\n",
    "        'runtime': runtime,\n",
    "        'inl_ratio': info['inlier_ratio'],\n",
    "    }\n",
    "relscalenet.cache.close()\n",
    "\n",
    "print(\"Estimated pose median inlier ratio:\", np.median([v['inl_ratio'] for v in pose_estimates.values()]))\n",
    "print(f\"Saving results to {pose_estimates_path}...\")\n",
    "write_hdf5(pose_estimates, pose_estimates_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6c25cf6bf44bc4bd987ccc99ea7ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC@5 = 21.34\n",
      "AUC@10 = 38.23\n",
      "AUC@20 = 53.42\n",
      "Avg. runtime: 14.6 ms.\n"
     ]
    }
   ],
   "source": [
    "AUC_THRESHOLDS = [5., 10., 20.]\n",
    "\n",
    "results = {\n",
    "    'r_err': [],\n",
    "    't_err': [],\n",
    "    'runtime': [],\n",
    "}\n",
    "\n",
    "with h5py.File(pose_estimates_path, \"r\") as f:\n",
    "    for k in tqdm(f.keys()):\n",
    "        pair = f[k]\n",
    "\n",
    "        R_gt = pair['R_gt'][()]\n",
    "        t_gt = pair['t_gt'][()]\n",
    "\n",
    "        if not ('R_est' in pair and 't_est' in pair):\n",
    "            results['r_err'].append(np.inf)\n",
    "            results['t_err'].append(np.inf)\n",
    "            continue\n",
    "\n",
    "        R_est = pair['R_est'][()]\n",
    "        t_est = pair['t_est'][()]\n",
    "        runtime = pair['runtime'][()]\n",
    "\n",
    "        r_err = geom.rotation_angle(R_est.transpose() @ R_gt)\n",
    "        t_err = geom.angle(t_est, t_gt)\n",
    "\n",
    "        results['r_err'].append(r_err)\n",
    "        results['t_err'].append(t_err)\n",
    "        results['runtime'].append(runtime)\n",
    "\n",
    "# Print results\n",
    "max_errs = np.max(np.c_[ results['r_err'], results['t_err']], axis=1)\n",
    "aucs = geom.pose_auc(max_errs, AUC_THRESHOLDS)\n",
    "aucs = [auc*100 for auc in aucs]\n",
    "avg_runtime = np.mean(results['runtime'])\n",
    "\n",
    "\n",
    "for i, t in enumerate(AUC_THRESHOLDS):\n",
    "    print(f'AUC@{int(t)} = {aucs[i]:.2f}')\n",
    "print(f'Avg. runtime: {avg_runtime*1000 :.1f} ms.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reldepthnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
